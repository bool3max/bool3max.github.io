<!DOCTYPE html>
<html lang="en-us">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title> A primer on the binary system: two&#39;s complement, IEEE-754 and more, explained | bool3max&#39;s blog</title>
  <meta name="description" content="I am currently a student. This blog is a collection of posts primarily about topics I&#39;m exploring or that I&#39;m interested in, some of them being quick references written for future use.">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="robots" content="all,follow">
  <meta name="googlebot" content="index,follow,snippet,archive">
  <meta property="og:title" content="A primer on the binary system: two&#39;s complement, IEEE-754 and more, explained" />
<meta property="og:description" content="Modern 64-bit processors are capable of storing $2^{64}$ different values within their general-purpose registers. Each one of those possible values is simply a different combination of 64 sequential bits - and each one of those bits can, of course, exist in one of two states at a single point in time.
Given any number of bits $ N $, we can always conclude that the maximum number of different values representable using that particular bit count is always $ 2^N $." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bool3max.win/posts/ieee754_explained/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-09-18T18:00:38+02:00" />
<meta property="article:modified_time" content="2021-09-18T18:00:38+02:00" />


  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="A primer on the binary system: two&#39;s complement, IEEE-754 and more, explained"/>
<meta name="twitter:description" content="Modern 64-bit processors are capable of storing $2^{64}$ different values within their general-purpose registers. Each one of those possible values is simply a different combination of 64 sequential bits - and each one of those bits can, of course, exist in one of two states at a single point in time.
Given any number of bits $ N $, we can always conclude that the maximum number of different values representable using that particular bit count is always $ 2^N $."/>

  
  
    
  
  
  <link rel="stylesheet" href="https://bool3max.win/css/style-white.css">
  
  
  
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  

  
<link rel="icon" type="image/png" href="https://bool3max.win/images/favicon.ico" />

  
  
</head>

<body class="max-width mx-auto px3 ltr">
  <div class="content index py4">

  <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;" aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
        <li><a href="/">Home</a></li>
         
        <li><a href="/posts/">All posts</a></li>
         
        <li><a href="https://github.com/bool3max/">GitHub</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li>
          <a class="icon" href=" https://bool3max.win/posts/man_and_pagers_plus_colors/" aria-label="Previous">
            <i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i>
          </a>
        </li>
        
        
        <li>
          <a class="icon" href="https://bool3max.win/posts/modifying_bitfields_of_datatypes/" aria-label="Next">
            <i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i>
          </a>
        </li>
        
        <li>
          <a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" aria-label="Top of Page">
            <i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i>
          </a>
        </li>
        <li>
          <a class="icon" href="#" aria-label="Share">
            <i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i>
          </a>
        </li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      
      <ul>
  
  
    
  
  
  <li>
    <a class="icon" href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fbool3max.win%2fposts%2fieee754_explained%2f" aria-label="Facebook">
      <i class="fab fa-facebook " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://twitter.com/share?url=https%3a%2f%2fbool3max.win%2fposts%2fieee754_explained%2f&text=A%20primer%20on%20the%20binary%20system%3a%20two%27s%20complement%2c%20IEEE-754%20and%20more%2c%20explained" aria-label="Twitter">
      <i class="fab fa-twitter " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fbool3max.win%2fposts%2fieee754_explained%2f&title=A%20primer%20on%20the%20binary%20system%3a%20two%27s%20complement%2c%20IEEE-754%20and%20more%2c%20explained" aria-label="Linkedin">
      <i class="fab fa-linkedin " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fbool3max.win%2fposts%2fieee754_explained%2f&is_video=false&description=A%20primer%20on%20the%20binary%20system%3a%20two%27s%20complement%2c%20IEEE-754%20and%20more%2c%20explained" aria-label="Pinterest">
      <i class="fab fa-pinterest " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="mailto:?subject=A%20primer%20on%20the%20binary%20system%3a%20two%27s%20complement%2c%20IEEE-754%20and%20more%2c%20explained&body=Check out this article: https%3a%2f%2fbool3max.win%2fposts%2fieee754_explained%2f" aria-label="Email">
      <i class="fas fa-envelope " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://getpocket.com/save?url=https%3a%2f%2fbool3max.win%2fposts%2fieee754_explained%2f&title=A%20primer%20on%20the%20binary%20system%3a%20two%27s%20complement%2c%20IEEE-754%20and%20more%2c%20explained" aria-label="Pocket">
      <i class="fab fa-get-pocket " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://reddit.com/submit?url=https%3a%2f%2fbool3max.win%2fposts%2fieee754_explained%2f&title=A%20primer%20on%20the%20binary%20system%3a%20two%27s%20complement%2c%20IEEE-754%20and%20more%2c%20explained" aria-label="reddit">
      <i class="fab fa-reddit " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.tumblr.com/share/link?url=https%3a%2f%2fbool3max.win%2fposts%2fieee754_explained%2f&name=A%20primer%20on%20the%20binary%20system%3a%20two%27s%20complement%2c%20IEEE-754%20and%20more%2c%20explained&description=Modern%2064-bit%20processors%20are%20capable%20of%20storing%20%242%5e%7b64%7d%24%20different%20values%20within%20their%20general-purpose%20registers.%20Each%20one%20of%20those%20possible%20values%20is%20simply%20a%20different%20combination%20of%2064%20sequential%20bits%20-%20and%20each%20one%20of%20those%20bits%20can%2c%20of%20course%2c%20exist%20in%20one%20of%20two%20states%20at%20a%20single%20point%20in%20time.%0aGiven%20any%20number%20of%20bits%20%24%20N%20%24%2c%20we%20can%20always%20conclude%20that%20the%20maximum%20number%20of%20different%20values%20representable%20using%20that%20particular%20bit%20count%20is%20always%20%24%202%5eN%20%24." aria-label="Tumblr">
      <i class="fab fa-tumblr " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fbool3max.win%2fposts%2fieee754_explained%2f&t=A%20primer%20on%20the%20binary%20system%3a%20two%27s%20complement%2c%20IEEE-754%20and%20more%2c%20explained" aria-label="Hacker News">
      <i class="fab fa-hacker-news " aria-hidden="true"></i>
    </a>
  </li>
</ul>

    </div>
    <div id="toc">
      <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#on-numbers-vs-values">On &ldquo;numbers&rdquo; vs &ldquo;values&rdquo;</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#positive-unsigned-integers">Positive (<em>unsigned</em>) integers</a></li>
    <li><a href="#signed-integers">Signed integers</a>
      <ul>
        <li><a href="#sign-and-magnitude">Sign and magnitude</a></li>
        <li><a href="#twos-complement">Two&rsquo;s complement</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#fixed-point-representation">Fixed-point representation</a></li>
    <li><a href="#ieee-754-floating-point-representation">IEEE-754 (floating-point representation)</a>
      <ul>
        <li><a href="#scientific-notation">Scientific notation</a></li>
        <li><a href="#encoding-in-memory">Encoding in memory</a></li>
        <li><a href="#precision-explained-in-detail">Precision explained in detail</a></li>
      </ul>
    </li>
  </ul>
</nav>
    </div>
  </span>
</div>


  <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
    <header>
      <h1 class="posttitle" itemprop="name headline">
        A primer on the binary system: two&#39;s complement, IEEE-754 and more, explained
      </h1>
      <div class="meta">
        
        <div class="postdate">
          
          <time datetime="2021-09-18 18:00:38 &#43;0200 CEST" itemprop="datePublished">2021-09-18</time>
          
        </div>
        
        
        
        
      </div>
    </header>

  
    <div class="content" itemprop="articleBody">
      <p>Modern 64-bit processors are capable of storing $2^{64}$ different <em>values</em> within their general-purpose registers. Each one of those possible <em>values</em> is simply a different combination of 64
sequential bits - and each one of those bits can, of course, exist in one of two states at a single point in time.</p>
<p>Given any number of bits $ N $, we can always conclude that the <em>maximum</em> number of different values representable using that particular bit count is always $ 2^N $.</p>
<p>Each one of those <em>values</em> can also be thought of as a different <em>number</em>. Us humans have designed the binary system as a consequence of needing to represent the many different numbers found in the decimal system -
the one <em>we</em> think and operate in. It turned out, however, that it&rsquo;s no small feat. As we&rsquo;ll see, representing positive (i.e. &ldquo;<em>unsigned</em>&quot;) integers is easy - but how do we represent negative integers? And what about non-integers (i.e. decimal, real numbers)?</p>
<h3 id="on-numbers-vs-values">On &ldquo;numbers&rdquo; vs &ldquo;values&rdquo;</h3>
<p>While we most often think of binary values as numbers, they can also simply represent different abstract ideas.
For example, think of the many syscall functions in the Linux kernel. They usually expose a &ldquo;<code>flags</code>&rdquo; argument - a value composed of one or more binary OR&rsquo;d &lsquo;flags&rsquo;, each of which is a value with exactly one set bit.
We never care what the decimal representation of the final flag argument is (after it has been &ldquo;fed&rdquo; the many other individual possible options), we simply use the macros provided by the appropriate header to construct the final value.
However nothing stops us from observing the decimal value of the final argument, and possibly using it in the future. I wouldn&rsquo;t call that good practice though. For example:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c" data-lang="c"><span style="color:#75715e">#include</span> <span style="color:#75715e">&lt;stdint.h&gt;</span><span style="color:#75715e">
</span><span style="color:#75715e">#include</span> <span style="color:#75715e">&lt;fcntl.h&gt;</span><span style="color:#75715e">
</span><span style="color:#75715e"></span>
<span style="color:#66d9ef">int32_t</span> fd1 <span style="color:#f92672">=</span> open(<span style="color:#e6db74">&#34;/some/file&#34;</span>, O_APPEND <span style="color:#f92672">|</span> O_CREAT <span style="color:#f92672">|</span> O_NONBLOCK),
        fd2 <span style="color:#f92672">=</span> open(<span style="color:#e6db74">&#34;/some/other/file&#34;</span>, <span style="color:#ae81ff">3136</span>);
</code></pre></div><p>Both <code>open()</code> calls receive <em>exactly</em> the same value as their second argument, i.e. the exact same set of flags, but there&rsquo;s no reason to ever use the second variant, unless you&rsquo;re doing code golf.</p>
<h1 id="representations">Representations</h1>
<p>Now, let&rsquo;s take a look into how different kinds of numbers in the decimal system are represented in binary form.
I will only be describing the systems/encodings in use today.</p>
<h2 id="positive-unsigned-integers">Positive (<em>unsigned</em>) integers</h2>
<blockquote>
<p>&ldquo;<em>un</em>signed&rdquo; as in - &ldquo;no sign&rdquo; - the value is absolute</p>
</blockquote>
<p>Representing exclusively positive integers in a binary format is trivial. Each of the bits in the sequence
carries a value twice as large as that of the previous, starting at $2^0$, or $1$. The most significant bit
carries the value $2^{N - 1}$, where $N$ is the total number of bits. The range of decimal values epresentable
using $N$ bits in this system ranges from $0$ to $2^{N - 1}$ (because we account for the case where all bits are zeroes).
To convert a value encoded in this binary format to a decimal value, we simply add values of all the &ldquo;set&rdquo; (i.e. &ldquo;$1$&quot;) bits together.</p>
<p>That&rsquo;s all that there really is to it. Here are some examples using 8-bit sequences, for the sake of clarity:</p>
<p>Note that, when representing a single byte sequence in textual form, the most significant
bit is most often writtten first.</p>
<p><code>0 0 0 0 1 0 0 0</code> - $8$ in decimal</p>
<p><code>1 0 0 0 0 0 0 0</code> - $128$ in decimal</p>
<p><code>1 1 0 0 1 0 0 1</code> - $201$ in decimal</p>
<h2 id="signed-integers">Signed integers</h2>
<p>So, how do we represent negative integers? From the get-go it is obvious that this is going to require more work - an encoding of some kind. Historically, there have existed several solutions to this problem.
I will describe two of them: the oldest and most trivial one - <strong>&ldquo;sign and magnitude&rdquo;</strong>, and the one currently in use in most modern systems - <strong>&ldquo;two&rsquo;s complement&rdquo;</strong>.</p>
<h3 id="sign-and-magnitude">Sign and magnitude</h3>
<p>In this method, the most significant bit denotes the sign of the value (+, positive ($0$) or -, negative ($1$)), while the remainder of the bits in the sequence represent the absolute value (<em>magnitude</em>), which is why it is called
<em>sign and magnitude</em>.</p>
<p>The magniute is encoded just as a regular unsinged integer. If we imagine an $N$-bit sequence, using &ldquo;sign and magnitude&rdquo; we can represent decimal values from $-2^{N - 1} - 1$ to $+2^{N - 1} - 1$, inclusive. For example, using an 8-bit sequence we can represent decimal values from
$-127$ to $+127$.</p>
<p>An intereseting property of &ldquo;sign and magnituide&rdquo; is that there are two ways to represent zero: <code>0 0 0 0 0 0 0 0</code> (+0) and <code>1 0 0 0 0 0 0 0</code> (-0), thus essentially wasting a value.
In addition, basic operations on &ldquo;sign and magnitude&rdquo;-encoded values (such as comparisons or even basic arithmetic) have to account for the sign bit, leading to them being computationally more expensive, which is not a
problem with the &ldquo;two&rsquo;s complement&rdquo; method, as we&rsquo;ll see.</p>
<h3 id="twos-complement">Two&rsquo;s complement</h3>
<p>In &ldquo;<a href="https://en.wikipedia.org/wiki/Two's_complement"><strong>two&rsquo;s complement</strong></a>&rdquo;, negative counterparts of a positive integer (and vice-versa) are obtained by:</p>
<ol>
<li>Performing a binary NOT operation on the whole bit sequence (i.e. reversing all of the bits)</li>
<li>Adding 1 to the previous result (regular binary addition)</li>
</ol>
<p>Another method that is easier to visualise is locating the first set (&quot;$1$&quot;) bit from the right, and then inverting all of the bits to the left of that bit.</p>
<p>Performing two&rsquo;s complement conversion on a negative number will, of course, convert it back to a positive one. The absolute value stays the same.</p>
<p><strong>Despite not relying solely on a single bit to denote the sign of the value, the highest-order bit still does just that - <code>0</code> for positive, and <code>1</code> for negative</strong>.</p>
<p>The range of decimal values representable using $N$ bits in &ldquo;two&rsquo;s complement&rdquo; ranges from $-2^{N - 1}$ to $+2^{N - 1} - 1$, (i.e. from $-128$ to $+127$ using an $8$-bit sequence). The range is not &ldquo;symmetric&rdquo; as we need
to represent the zero as well.</p>
<blockquote>
<p>Seeing as we can encode (using 8 bits, as an example) positive values from $0$ to $+127$, and seeing as $+127$&rsquo;s bit pattern is the following: <code>0 1 1 1 1 1 1 1</code>, it is apparent why we cannot further encode larger positive values: it would require us to
flip the highest order bit to a $1$, <em>which would no longer denote a positive value</em>. That&rsquo;s why the upper range is capped where all bits but the most significant one are set (&quot;$1$&quot;) - i.e. the value $2^{N - 1} - 1$.</p>
</blockquote>
<p>Two&rsquo;s complement solves many of the problems of the methods that came before it:</p>
<ul>
<li>there&rsquo;s only one way to represent a zero (<code>0 0 0 0 0 0 0 0</code>)
<ul>
<li>again, a side effect of this is that it encodes 1 more possible (than other solutions) value on either side of the range</li>
<li>utilizing 8 bits sign and magnitude encodes values from $-127$ to $+127$, but two&rsquo;s complement encodes values from $-128$ to $+127$</li>
</ul>
</li>
<li><strong>arithmetic operations are trivial and require no additional computation - for example, adding 2 two&rsquo;s-complement-encoded values requires no operation bar performing regular binary addition - <em>regardless of the operands' signs</em></strong></li>
</ul>
<p>Below are a couple step-by-step examples of converting an unsigned integer to its negative two&rsquo;s complement countepart:</p>
<ul>
<li>
<p><code>0 1 0 0 0 0 1 0</code> ($66$)</p>
<ol>
<li>Peform a binary NOT on the bit sequence: <code>1 0 1 1 1 1 0 1</code></li>
<li>Add $1$ to the previous result: <code>1 0 1 1 1 1 1 0</code></li>
<li>$-66$</li>
</ol>
</li>
<li>
<p><code>0 0 0 0 0 0 0 1</code> ($1$)</p>
<ol>
<li>Perform a binary NOT on the bit sequence: <code>1 1 1 1 1 1 1 0</code></li>
<li>Add $1$ to the previous result: <code>1 1 1 1 1 1 1 1</code></li>
<li>$-1$</li>
</ol>
</li>
</ul>
<p>The process is of course exactly the same regardless of the length of the bit sequence (e.g. $32$, $64$, etc&hellip;)</p>
<h4 id="the-most-negative-number">The most negative number</h4>
<p>The most negative number representable using two&rsquo;s complement in an $N$-bit sequence is a special case. We know that applying the two&rsquo;s complement operation on <em>any</em> valid bit sequence will flip its sign, i.e.
$-5$ would become $+5$, $+77$ would become $-77$, $-112$ would become $+112$ and so on. The same is not true for the most negative number - $-2^{N - 1}$. If we take an 8-bit sequence as an example, we know that it can
represent decimal numbers ranging from $-128$ to $+127$ (exactly why - it is explained above). So what would happen if we apply two&rsquo;s complement (i.e. we try to negate) the bit sequence representing
$-128$? We would expect to obtain $+128$ as the result - but we know that it cannot be represented (it is by 1 larger than the largest representable number). So what do we get? We get the exact same bit sequence.
We can see this in action, if we apply two&rsquo;s complement on the lowest number in an 8-bit system, i.e. $-128$:</p>
<ol>
<li><code>1 0 0 0 0 0 0 0</code> ($-128$ in two&rsquo;s complement)</li>
<li><code>0 1 1 1 1 1 1 1</code> (reversing the bits)</li>
<li><code>1 0 0 0 0 0 0 0</code> (after adding $1$) - the same bit sequence that we started with!</li>
</ol>
<p>Thus, the <a href="https://en.wikipedia.org/wiki/Two%27s_complement#Most_negative_number">most negative number</a> is a special case in the two&rsquo;s complement system, and it has to be treated with special care.
The C and C++ standards in particular do not define behavior when dealing with the most negative number. As an example, the following C snippet (on an x86-64 machine), produces <code>-128</code> as the result:
(as is, well, expected in this special case):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c" data-lang="c"><span style="color:#66d9ef">signed</span> <span style="color:#66d9ef">char</span> b <span style="color:#f92672">=</span> (<span style="color:#66d9ef">signed</span> <span style="color:#66d9ef">char</span>) <span style="color:#f92672">-</span><span style="color:#ae81ff">128</span> <span style="color:#f92672">*</span> (<span style="color:#66d9ef">signed</span> <span style="color:#66d9ef">char</span>) <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>;
</code></pre></div><h1 id="real-numbers">Real numbers</h1>
<blockquote>
<p>Floating-point arithmetic is considered an esoteric subject by many people.
-David Goldberg</p>
</blockquote>
<p>Useful resources and tools:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/IEEE_754-1985">IEEE 754-1985</a></li>
<li><a href="https://en.wikipedia.org/wiki/Floating-point_arithmetic">Floating-point arithmetic</a></li>
<li><a href="https://en.wikipedia.org/wiki/Single-precision_floating-point_format">Single-precision floating-point format</a></li>
<li><a href="https://en.wikipedia.org/wiki/Scientific_notation">Scientific notation</a></li>
<li><a href="https://fabiensanglard.net/floating_point_visually_explained/">Floating Point Visually Explained</a></li>
<li><a href="https://youtu.be/gc1Nl3mmCuY">&ldquo;Floating Point Numbers&rdquo; - NERDfirst (17:29)</a></li>
<li><a href="https://www.h-schmidt.net/FloatConverter/IEEE754.html">IEEE-754 Floating Point Converter</a></li>
</ul>
<p>It&rsquo;s immediately apparent that we need yet another encoding in order to represent and operate on non-integer (real) numbers.</p>
<h2 id="fixed-point-representation">Fixed-point representation</h2>
<p>The most trivial way to represent real numbers using a binary format is by employing a &ldquo;fixed point representation&rdquo;. That is, in an N-bit pattern, choose a position
between two adjecent bits in which the radix point will be placed. Bits to the left of the radix point (also called the <em>binary point</em>) represent the whole (integer)
part of the number, while bits to the right of it represent the fractional part of the number. Starting from the radix point, bits to the right of it (i.e. the least significant half of the bits) represent
<strong>negative powers of 2</strong>. The highest order bit of the fractional part of the bit pattern represents the value $2^{-1}$ ($0.5$), while the lowest order bit represents the value
$2^{-(N / 2)}$, where $N$ is the number of bits in the entire bit pattern. This of course applies to bit patterns of any length.</p>
<p>The most glaring issue of this system is that the magnitude of the integer part as well as the degree of precision of the fractional part are <strong>fixed</strong> and dependent
on the position of the binary point. If we take an 8-bit wide sequence as an example, and if we assume that the binary point is fixed exactly in the middle - then, its
integer part ranges from $0$ through $15$ (though two&rsquo;s complement could be utilized here to provide negative number representation), and its fractional part ranges from
$0.0625$ to $0.9375$. If we however at any point begin to desire more precision in the integer part (for example we need to store the value $38$) at the cost of less
precision in the fractional part, we are out of luck, as the radix point&rsquo;s position is <strong>fixed</strong>.</p>
<p>The other major problem is that fractional values that aren&rsquo;t combinations of integer powers of 2 are <strong>unrepresentable</strong>. To illustrate this, let&rsquo;s imagine $0.2$ (or $\frac{1}{5}$). It cannot
be represented as a power of two, nor as a combination of any number of powers of two. In contrast, $0.75$ for example, is representable as $2^{-1} + 2^{-2}$.
So how do we represent those values? <strong>Well, we cannot</strong>. This problem is unfortunately not solved by any other more advanced representation, as we&rsquo;ll see later. The best
we can do is <em><strong>approximate</strong></em> those values. The more bits we have to work with, the better our <em><strong>precision</strong></em> is, and the &ldquo;closer&rdquo; we can get to the desired, ultimately irrepresentable value.
For example, using an 8-bit fixed-point representation sequence where the radix point is placed in the middle, the closest we can get to $0.2$ is <code>0000.0011</code>, or $0.1875$, i.e. $ 2^{-4} + 2^{-3} = \frac{1}{16} + \frac{1}{8}$.
As the bit count increases, so does our precision, and we get closer and closer to $0.2$, but we never quite get there. The closest we can get to $0.35$ is <code>0000.0110</code>,
or $0.375$. Note that by &ldquo;close&rdquo; I don&rsquo;t necesserily mean &ldquo;just before&rdquo; or &ldquo;slighly less&rdquo; - <strong>the closes possible approximation may also be larger than the desired number</strong>.</p>
<p>This issue is actually present in the decimal number system that we use every day: in it, we cannot exactly represent a value such as $\frac{1}{3}$ (one third). We usually <em>approximate</em> it by writing it as $0.33333&hellip;.$, with the digits
after the decimal point repeating &ldquo;forever&rdquo; (often indicated by placing a dot mark above last written digit).</p>
<p>To get around the first issue of fixed-point representations, a <strong>floating-point</strong> representation can be used. The name is quite self explanatory - the binary point
is no longer fixed to a specific position and can instead &ldquo;float&rdquo; around on demand. This allows for tradeoffs: a larger magnitude for the integer part of a number
with lower fractional precision, or vice versa. The industry standard for representing floating point numbers in computers is the <a href="https://en.wikipedia.org/wiki/IEEE_754-1985">IEEE-754</a> standard, first introduced
in 1985.</p>
<h2 id="ieee-754-floating-point-representation">IEEE-754 (floating-point representation)</h2>
<p>The standard was first introduced in 1985 and describes how floating point numbers of various widths are to be stored in memory, as well as how various
arithmetic operations are to behave on those numbers. While all of the semantics of the standard could be implemented in software, it was commonly implemented in hardware
by so called FPUs (&ldquo;floating-point units&rdquo;) for the sake of efficiency and speed. The first piece of hardware to implement IEEE-754 is the <a href="https://en.wikipedia.org/wiki/Intel_8087">Intel 8087</a>
floating-point co-processor. In modern processors, IEEE-754 operations are implemented directly within, ridding the need for external FPUs.</p>
<p>The 2008 revision of the standard can be found <a href="https://irem.univ-reunion.fr/IMG/pdf/ieee-754-2008.pdf">here</a>, though it is of course very technical and thorough.</p>
<h3 id="scientific-notation">Scientific notation</h3>
<p>Representation of floating point numbers as described by IEEE-754 shares a lot of semantics with scientific notation, which I will first describe.</p>
<p><strong>Scientific notation</strong> is a way of representing numerical values in the decimal number system, that are way too long to be conveniently written down in their entirety.
The general form is this:</p>
<p>$$M \times 10^N $$</p>
<p>, where $M$ is a real number called the <strong>mantissa</strong>/<strong>significand</strong>, and $N$ is an integer (whole number) called the <strong>exponent</strong> (this terminology
comes into use again later). If $M$ is negative, it is prefixed with a minus sign.</p>
<p>Examples:</p>
<ul>
<li>$88 120 000 000$ in decimal notation can be written as $8.8812 \times 10^{11}$ in scientific notation.</li>
<li>$-67 000$ in decimal notation can be written as $-6.7 \times 10^4$ in scientific notation</li>
<li>$0.000129$ in decimal notation can be written as $1.29 \times 10^{-4}$</li>
</ul>
<p>While the mantissa <em>m</em> can generally be any real number, most commonly a subset of the scientific notation called <strong>normalized notation</strong> is used, whereby
the absolute value of the mantissa is constrained between 1 and 10, i.e. $1 \leq |M| \lt 10$.</p>
<p>Meaning that, while $0.000129$ can be written as $12.9 \times 10^{-5}$, or even $1290 \times 10^{-7}$, in <strong>normalized notation</strong> the mantissa is constrained to $1.29$, the exponent
is therefore chosen accordingly to be $-4$, and the final expression ends up being $1.29 \times 10^{-4}$.</p>
<p>The degree of precision in the final value depends of course on the number of significant digits of the mantissa. For example, the significand $1.2345$ allows for
precise values such as $0.012345$ or $0.12345$, while a mantissa with less significant digits, for example $1.234$ would allow for lesser degrees of precion such as $0.01234$ or $0.1234$.
This is explained in a bit more detail <a href="https://en.wikipedia.org/wiki/Scientific_notation#Significant_figures">here</a>.</p>
<hr>
<p>Now back to IEEE-754. The reason that I brought up scientific notation in the first place is that real numbers in computers are represented in much a similar way
, as we&rsquo;ll see.</p>
<p>The standard defines 4 levels of precision, however the two most commonly used and implemented ones are:</p>
<ul>
<li><strong>single-precision</strong>: 32 bits - referred to as a <code>float</code> by the C standard</li>
<li><strong>double-precision</strong>: 64 bits - referred to as a <code>double</code> by the C standard</li>
</ul>
<h3 id="encoding-in-memory">Encoding in memory</h3>
<p>For the sake of efficacy I will be presenting all examples in the <em>single precision</em> format. The double precision follows exactly the same rules, however as the bit count
is increased some constants used in calculations differ. I will note those along the way.</p>
<p>The bitfields of floating point numbers encoded as per IEEE-754 are split into <strong>3</strong> parts:</p>
<ul>
<li><strong>1</strong> sign bit</li>
<li><strong>8</strong> biased exponents bits (11 in double-precision)</li>
<li><strong>23</strong> mantissa bits (52 in double-precision)</li>
</ul>
<p><img src="https://fabiensanglard.net/floating_point_visually_explained/floating_point_layout.svg" alt="img"></p>
<p><img src="https://fabiensanglard.net/floating_point_visually_explained/floating_point_math.svg" alt="img"></p>
<p>The final value comprised of these parts can be calculated as follows:</p>
<p>$$(-1)^s \times (1 + \text{mantissa}) \times 2^{\text{exponent} - 127}$$</p>
<p>The most significant bit - <strong>the sign bit</strong> denotes the sign of the final number. $0$ for positive and $1$ for negative. As a result of this, two zero values are representable: $+0$ and $-0$.</p>
<p>The next 8 bits represent the <strong>biased exponent</strong> that is used to scale the value of the mantissa. The biased exponent is treated as a regular 8-bit unsigned integer, however
it is $127$-biased (the stored value is increased or &ldquo;shifted&rdquo; by 127) to allow for storage of negative exponents. To obtain the actual value of the exponent, subtract $127$ from its value, e.g. a biased exponent of
$231$ would denote an actual exponent of $231-127 = 104$. A bias is used instead of an encoding such as &ldquo;two&rsquo;s complement&rdquo; for efficient comparisons. Once the bias is
taken into account, the value of the exponent ranges from $-126$ to $+127$ (<strong>not</strong> $-127$ to $128$ as you may believe - cases where the exponent field is all $0$s / all $1$s are treated specially, more on this later).</p>
<p>The last 23 bits represent the <strong>mantissa</strong> (analogous to a <em>significand</em> in scientific notation). Its bits represent negative powers of two, starting with $2^{-1}$
at the most significant bit, all the way through to $2^{-23}$ at the least significant bit. A non-existent $1$-bit carrying the value $2^0$ ($1.0$) is implied to be
placed just between the exponent and mantissa bits - that&rsquo;s what the $+1$ denotes in the equation pictured above, and is also why the mantissa is guaranteed to be
between $1.0$ and $2.0$ in value, i.e. $1.0 \le mantissa \lt 2.0$. This is called the &ldquo;leading bit convention&rdquo; or the &ldquo;implicit bit convention&rdquo;, and it allows the format
to have an extra bit of precision. This is true in all cases but when dealing with a <em>denormalized</em> number, indicated by <strong>the biased exponent field being all 0s</strong> - more on this later.</p>
<p>Once all three components are known obtaining the final value is quite trivial - the formula pictured above is quite self-explanatory.
<a href="https://fabiensanglard.net/floating_point_visually_explained/">This article</a> provides a nice additional visual explanation.
<strong>IEEE-754 representations in no way solve the representation issues discussed in the earlier</strong>. If the value cannot be represented by <strong>a combination of any powers of two
ranging from -1 to -23 increased by 1.0 and then scaled (multiplied) by any power of two ranging from -126 to 127</strong>, then it cannot be <em>exactly</em> represented, and is instead <em><strong>approximated</strong></em> as closely as possible.
This stands for integer numbers as well - for example, the value $16,777,217$ cannot be exactly represented by a single-precision IEEE-754 format, and is instead
rounded down to $16,777,216$. Perhaps somewhat counter-intuitively, the value $5$ <em>can</em> be exactly represented. By setting the exponent field to $2$, and the mantissa field to $0.25$ (i.e. setting the second most significant bit carrying the value $2^{-2}$ or $0.25$),
the final equation becomes $1 \times 2^2 \times (1 + 0.25) = 1.25 \times 4 = 5$.</p>
<h4 id="representing-zeroes">Representing zeroes</h4>
<p>Zeroes are represented by setting the exponent and mantissa fields to all $0$s. The sign bit determines if the zero is positive ($+0$) or negative ($-0$).</p>
<h4 id="denormalized-numbers">Denormalized numbers</h4>
<p>When the exponent portion of a number is all $0$s the value is said to be <em>denormalized</em>. This comes with a couple deviations from the
regular rules about representing floating point values (and as such the general formula cannot be used in that case). Firstly, the exponent&rsquo;s actual value (when the 127-bias is taken into account)
isn&rsquo;t $-127$ as you may expect, but rather $-126$, just as if the biased exponent was $1$. Secondly, the implied $1$-bit no longer exists,
leading to the mantissa being constrained between $0.0$ and $1.0$, i.e. $0.0 \le mantissa \lt 1.0$. As the leading digit of the mantissa
is a 0, this allows for representation of numbers even smaller than the smallest normalized number. For example, the numbers closest to $0$
representable using a normalized single-precision format are $\pm2^{-126}$ ($\approx\pm1.17549 \times 10^{-38}$) - this is the case when the mantissa&rsquo;s bits
are all $0$s and the biased exponent is $1$. In contrast, the numbers closest to $0$ representable when utilizing denormalized values
are $\pm2^{-23} \times 2^{-126}$ ($\approx\pm1.40130 \times 10^{-45}$) - this being the case when the mantissa&rsquo;s least significant bit is set, and when the
exponent&rsquo;s bits are of course all $0$s. Any non-zero number with magnitude smaller than the smallest representable normalized
number is consider to be <strong>subnormal</strong>. The existence of denormalized numbers guarantees that there is always a non-zero
difference between any two nearby floating point numbers, leading to operations such as $x - y$ never being able to produce a zero result
despite the operands not being equal.</p>
<h4 id="other-special-values">Other special values</h4>
<p>Being that the IEEE-754 standard also requires conforming implementations to support a number of arithmetic operations (e.g. taking the square
root of a value), and being that results of many of those operations are often invalid or undefined with certain operands, the standard defines five <strong>exceptions</strong>.
The results of operations which produce exceptions are often one of three special values: NaN (&ldquo;not-a-number&rdquo;), +infinity (+∞), and -infinity (-∞).
Their bitfield representations are listed in the table below:</p>
<table>
<thead>
<tr>
<th style="text-align:center">Type</th>
<th style="text-align:center">Sign bit</th>
<th>Biased exponent</th>
<th>Mantissa</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">NaN</td>
<td style="text-align:center">any</td>
<td>all 1s</td>
<td>any non-zero</td>
</tr>
<tr>
<td style="text-align:center">+∞</td>
<td style="text-align:center">0</td>
<td>all 1s</td>
<td>all 0s</td>
</tr>
<tr>
<td style="text-align:center">-∞</td>
<td style="text-align:center">1</td>
<td>all 1s</td>
<td>all 0s</td>
</tr>
</tbody>
</table>
<p>Mathematically undefined operations, such as taking the square root of a negative number for example, result in a NaN (e.g. $\sqrt{-5.0}$).
Divisions by zero for example result in ±∞ depending on the sign of the first operand (at least that&rsquo;s what I&rsquo;ve observed on my system).</p>
<h3 id="precision-explained-in-detail">Precision explained in detail</h3>
<p>The larger the absolute magnitude of the integer part of a number is, the less precision is available to cover the fractional part of the number. This is illustrated
by this initially quite elusive (at least to me) graph found on the IEEE-754 Wikipedia page, pictured below:</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/18/IEEE754.svg/1024px-IEEE754.svg.png" alt="img"></p>
<p>To understand why this is, let&rsquo;s recap what roles the parts of an IEEE-754 format actually play.</p>
<p><strong>The exponent</strong> tells us between which two consecutive
powers of 2 our value falls within - for example $[0.25, 0.5]$ (the value falls between powers of two $-2$ and $-1$), $[1, 2]$ (the value falls between powers of two
$0$ and $1$), $[16, 32]$ (the value falls between powers of two $4$ and $5$), all the way up to $[2^{127}, 2^{128}]$. If the exponent is for example $18$, we know
that the final value will fall somewhere between $[2^{18}, 2^{19}]$ or $[262144, 524288]$.</p>
<p><strong>The mantissa</strong> then <em>dividies</em> that window/range indicated by the exponent into $2^{23}$ (or $8388608$) &ldquo;buckets&rdquo;, or other individual values.
In other words, it approximates the exact location/offset of the final number in the range/window indicated by the exponent. Since the mantissa is constrained
between $1.0$ and $2.0$ in normalized form, and since the final value is obtained by multiplying the exponent and mantissa, <strong>it is clear why
the exponent only stores the lower end of the range</strong>. The closer the mantissa is to $1.0$ the closer the final number is to the lower end of the range,
and the closer it is to $2.0$ the closer the final number is to the upper end of the range.</p>
<p><strong>The shorter the range within which the final number falls in, the more &lsquo;<em>precisely</em>&rsquo; its location/offset within that range can be approximated</strong>, because
each of the $8388608$ buckets dividing the range represent a smaller value. For example, imagine that we are trying to represent a number in the range
$[1, 2]$. Our range in that case is $(2 - 1) = 1$, and each of the different $8388608$ mantissa representations dividing that range carry the value $\frac{2 - 1}{8388608}$, or $0.00000011920928955078125$ - <strong>that value being our <em>precision</em></strong> -
i.e. the <strong>smallest difference between two consecutive numbers in that range</strong>.</p>
<p>In contrast, had we instead wanted to represent a value in the range $[512, 1024]$ (so $9$ being the exponent), the precision would &lsquo;fall down&rsquo; to $\frac{1024 - 512}{8388608}$, or $0.00006103515625$. Quite a bit less precise.</p>
<p>Let&rsquo;s see an example which showcases the precision in action: say we want to represent $65536$. It is very trivial to represent exactly that value only
using the exponent, as it is a direct power of two. We simply set the sign to $0$, the exponent to $16$ (or $143$ when accounting for the bias), and we leave
the mantissa at $0$. Now, the exponent&rsquo;s value tells us that we can represent values in the range $[2^{16}, 2^{17}]$, or $[65536, 131072]$ - essentially we are covering a range of $65536$ values.
Our precision in that case is: $\frac{65536}{8388608} = 0.0078125$, or $7.8125\text{e-}3$. If we then flip the least significant bit of the mantissa on (the one carrying the value $2^{-23}$),
the final value becomes: $+1 \times 2^{16} \times (1 + 2^{-23}) = 65536.0078125$. This value is the <em>next largest representable one</em> after $65536$, and it is larger than it <em><strong>exactly</strong></em> by the value of the precision in that range (i.e. $7.8125\text{e-}3$).
After it, the next largest value is: $+1 \times 2^{16} * (1 + 2^{-22}) = 65536.015625$. Again, the difference between it and the previous value is the precision: $65536.015625 - 65536.0078125 = 0.0078125$.
Past $65536$, we cannot make jumps more precise/smaller than $7.8125\text{e-}3$.</p>
<p>From the graph pictured above we can observe that (at least concerning the 32-bit single-precision format) somewhere between $10^6$ and $10^8$ the precision reaches $10^0 = 1$, meaning that past that particular cutoff point,
we lose the ability to represent non-integer numbers, as our precision itself becomes a whole number without a fractional part. As far as the double-precision
format goes, that cutoff point comes a lot later (it isn&rsquo;t even visible on the graph).</p>

    </div>
  </article>

  
  





  <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/posts/">All posts</a></li>
         
          <li><a href="https://github.com/bool3max/">GitHub</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#on-numbers-vs-values">On &ldquo;numbers&rdquo; vs &ldquo;values&rdquo;</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#positive-unsigned-integers">Positive (<em>unsigned</em>) integers</a></li>
    <li><a href="#signed-integers">Signed integers</a>
      <ul>
        <li><a href="#sign-and-magnitude">Sign and magnitude</a></li>
        <li><a href="#twos-complement">Two&rsquo;s complement</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#fixed-point-representation">Fixed-point representation</a></li>
    <li><a href="#ieee-754-floating-point-representation">IEEE-754 (floating-point representation)</a>
      <ul>
        <li><a href="#scientific-notation">Scientific notation</a></li>
        <li><a href="#encoding-in-memory">Encoding in memory</a></li>
        <li><a href="#precision-explained-in-detail">Precision explained in detail</a></li>
      </ul>
    </li>
  </ul>
</nav>
    </div>

    <div id="share-footer" style="display: none">
      
      <ul>
  
  
    
  
  
  <li>
    <a class="icon" href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fbool3max.win%2fposts%2fieee754_explained%2f" aria-label="Facebook">
      <i class="fab fa-facebook fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://twitter.com/share?url=https%3a%2f%2fbool3max.win%2fposts%2fieee754_explained%2f&text=A%20primer%20on%20the%20binary%20system%3a%20two%27s%20complement%2c%20IEEE-754%20and%20more%2c%20explained" aria-label="Twitter">
      <i class="fab fa-twitter fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fbool3max.win%2fposts%2fieee754_explained%2f&title=A%20primer%20on%20the%20binary%20system%3a%20two%27s%20complement%2c%20IEEE-754%20and%20more%2c%20explained" aria-label="Linkedin">
      <i class="fab fa-linkedin fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fbool3max.win%2fposts%2fieee754_explained%2f&is_video=false&description=A%20primer%20on%20the%20binary%20system%3a%20two%27s%20complement%2c%20IEEE-754%20and%20more%2c%20explained" aria-label="Pinterest">
      <i class="fab fa-pinterest fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="mailto:?subject=A%20primer%20on%20the%20binary%20system%3a%20two%27s%20complement%2c%20IEEE-754%20and%20more%2c%20explained&body=Check out this article: https%3a%2f%2fbool3max.win%2fposts%2fieee754_explained%2f" aria-label="Email">
      <i class="fas fa-envelope fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://getpocket.com/save?url=https%3a%2f%2fbool3max.win%2fposts%2fieee754_explained%2f&title=A%20primer%20on%20the%20binary%20system%3a%20two%27s%20complement%2c%20IEEE-754%20and%20more%2c%20explained" aria-label="Pocket">
      <i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://reddit.com/submit?url=https%3a%2f%2fbool3max.win%2fposts%2fieee754_explained%2f&title=A%20primer%20on%20the%20binary%20system%3a%20two%27s%20complement%2c%20IEEE-754%20and%20more%2c%20explained" aria-label="reddit">
      <i class="fab fa-reddit fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.tumblr.com/share/link?url=https%3a%2f%2fbool3max.win%2fposts%2fieee754_explained%2f&name=A%20primer%20on%20the%20binary%20system%3a%20two%27s%20complement%2c%20IEEE-754%20and%20more%2c%20explained&description=Modern%2064-bit%20processors%20are%20capable%20of%20storing%20%242%5e%7b64%7d%24%20different%20values%20within%20their%20general-purpose%20registers.%20Each%20one%20of%20those%20possible%20values%20is%20simply%20a%20different%20combination%20of%2064%20sequential%20bits%20-%20and%20each%20one%20of%20those%20bits%20can%2c%20of%20course%2c%20exist%20in%20one%20of%20two%20states%20at%20a%20single%20point%20in%20time.%0aGiven%20any%20number%20of%20bits%20%24%20N%20%24%2c%20we%20can%20always%20conclude%20that%20the%20maximum%20number%20of%20different%20values%20representable%20using%20that%20particular%20bit%20count%20is%20always%20%24%202%5eN%20%24." aria-label="Tumblr">
      <i class="fab fa-tumblr fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fbool3max.win%2fposts%2fieee754_explained%2f&t=A%20primer%20on%20the%20binary%20system%3a%20two%27s%20complement%2c%20IEEE-754%20and%20more%2c%20explained" aria-label="Hacker News">
      <i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i>
    </a>
  </li>
</ul>

    </div>

    <div id="actions-footer">
      
        <a id="menu-toggle" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;" aria-label="Menu">
          <i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc-toggle" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;" aria-label="TOC">
          <i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share-toggle" class="icon" href="#" onclick="$('#share-footer').toggle();return false;" aria-label="Share">
          <i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" aria-label="Top of Page">
          <i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>


  <footer id="footer">
  <div class="footer-left">
    Copyright  &copy; 2022  Bogdan Mitrović 
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
        <li><a href="/">Home</a></li>
         
        <li><a href="/posts/">All posts</a></li>
         
        <li><a href="https://github.com/bool3max/">GitHub</a></li>
        
      </ul>
    </nav>
  </div>
</footer>


  </div>
</body>

<link rel="stylesheet" href=/lib/font-awesome/css/all.min.css>
<script src=/lib/jquery/jquery.min.js></script>
<script src=/js/main.js></script>


  


<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

</html>
